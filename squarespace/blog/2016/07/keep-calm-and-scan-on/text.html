<p><span style="color: rgb(34, 34, 34)">BY: JEANETTE MUMFORD, CYRIL PERNET, THOMAS YEO, LISA NICKERSON, NILS MUHLERT, NIKOLA STIKOV, RANDY GOLLUB, &amp; OHBM COMMUNICATIONS COMMITTEE (IN CONSULTATION WITH THOMAS NICHOLS)  </span></p>
<p><span style="color: rgb(34, 34, 34)">In recent weeks a lot of attention has been given to the </span><a href="http://www.pnas.org/content/113/28/7900.full"><span style="color: rgb(17, 85, 204)">paper</span></a><span style="color: rgb(34, 34, 34)"> “Cluster failure: Why fMRI inferences for spatial extent have inflated false-positive rates”, by Eklund, Nichols and Knutsson, in the Proceedings of the National Academy of Sciences.  This work highlights an important concern; however, some of the media attention has been based on a misunderstanding and an ‘inflated’ interpretation of the results.  Specifically, too much weight has been given to the numbers “40,000 impacted studies” and “70% false positives”, an unfortunate side effect of reducing a study rich in information to a few soundbites.  We respect the views of this paper and the effort put forth by the authors who, like the leadership of OHBM, understand there is a </span><a href="http://www.humanbrainmapping.org/files/2016/COBIDASreport.pdf"><span style="color: rgb(17, 85, 204)">growing concern for validity and reproducibility in our field</span></a><span style="color: rgb(34, 34, 34)">. The purpose of this post is to put these numbers in context and clarify how these findings impact our view of past and future fMRI results.</span></p>
<p><span style="color: rgb(102, 102, 102)">Background:</span><br><span style="color: rgb(34, 34, 34)">In task-based fMRI studies we are often interested in </span><span style="color: rgb(51, 51, 51)">looking for systematic differences between experimental conditions or cognitive states</span> across upwards of 100,000 voxels in the brain.<span style="color: rgb(34, 34, 34)"> It is widely known that this large number of statistical tests, typically one per voxel, requires correction for multiplicity. The most common approaches focus on control of the family-wise error (FWE), which is the probability that a given study will produce any false positives.  The most common approaches for FWE control are voxel-wise and cluster-wise thresholding. Voxel-wise thresholding draws conclusions about specific voxels and cluster-wise thresholding allows one to conclude whether a group (or cluster) of adjacent voxels show an effect based on a feature, most often its size (e.g. only groups of voxels bigger than size N are significant). </span><span style="color: rgb(51, 51, 51)">Eklund et al. consider both voxel-wise and cluster-wise FWE control in an exercise that tests whether the thresholding methods and their implementation by various software packages control the FWE as advertised. The innovation in this work is that they used resting-state fMRI data rather than computer generated simulation data to estimate noise (see below for more on this); they analyzed this resting-state data as if it were actually task fMRI data. </span></p>
<p><span style="color: rgb(51, 51, 51)">Eklund et al. find voxel-wise results are always correct, i.e. control FWE below a requested 5% level, and are thus safe; we won't discuss these further. They also find that, depending on the exact methods and tools used, cluster-wise results can be invalid, i.e. have FWE in excess of the traditionally accepted 5% level.  Understanding the specifics of when these methods are invalid is the focus of the article.</span></p>
<p><span style="color: rgb(34, 34, 34)">A cartoon example of the cluster-wise based strategy is illustrated in Figure 1. First, a primary threshold is required to define clusters (in Eklund et al. this is called a cluster-defining threshold, CDT). The CDT is typically based on the uncorrected voxelwise p-values.  SPM and FSL use random field theory to obtain FWE-corrected p-values, which requires an estimate of the spatial smoothness </span><span style="color: rgb(51, 51, 51)">of the image that is being thresholded, typically a map of t-statistics that quantifies the effect size at each voxel</span><span style="color: rgb(34, 34, 34)">. AFNI uses a simulation-based procedure that also relies on a smoothness estimate. In contrast, another choice is to use a permutation approach, which is based on randomly permuting data labels to generate a null distribution for cluster size that is used to compute a p-value. The approaches in the 3 widely used fMRI data analysis packages, SPM, FSL and AFNI, are variations of parametric methods, and are based on specific assumptions about the data, while the permutation method is nonparametric and requires minimal assumptions. </span><span style="color: rgb(67, 67, 67)"><strong><span class="sqsrte-large">What is unique about this work?</span></strong></span><span style="color: rgb(34, 34, 34)">This paper is an example of a simulation study, an evaluation of a method based on ‘made up’ data. The reason simulations are used is because quantifying FWE can only be done if the ground truth is known. Specifically, we must ensure there is no signal in the data. A simulation is most useful when the simulated data reflect what we would find in real data as closely as possible. This has been a limitation of previous studies, which generated synthetic data with software and used this synthetic data to test the performance of the analysis algorithms (</span><a href="http://www.ncbi.nlm.nih.gov/pubmed/?term=Assessing+the+significance+of+focal+activations+using+their+spatial+extent."><span style="color: rgb(17, 85, 204); font-weight: 700">Friston et al. (1994)</span></a><span style="color: rgb(34, 34, 34)"> and</span><a href="http://www.ncbi.nlm.nih.gov/pubmed/14683734"><span style="color: rgb(34, 34, 34)"> </span><span style="color: rgb(17, 85, 204); font-weight: 700">Hayasaka and Nichols (2003)</span></a><span style="color: rgb(34, 34, 34)"> are examples). This work uses a large pool of real human resting state fMRI data as a source of null data, or data that do not contain any task-related signal. Fitting a model of a task to the data should not find any activation. The advantage of using actual fMRI data is that the spatial and temporal structure of the noise is real, in contrast to previous simulation studies that used computer-generated null data.  In the simulations in Eklund et al., random samples of subjects from the resting state data set are taken, and these samples are analyzed with a fake task design. The subject-specific task activation estimates are then entered into either a 1-sample test (to test the hypothesis that there is an effect of this task in this group) or a 2-sample test between two groups of subjects (to test the hypothesis that the effect of the task differs between the groups). Each result is assessed in the usual way, looking for FWE-corrected p-values that fall below p=0.05, and the occurrence of significant clusters (cluster-wise approach) is recorded. The authors repeat this a total of 1000 times and the FWE is computed as the number of simulated studies with any false positives divided by 1000. In theory, using p=0.05 should result in a FWE of 5%. </span><span style="color: rgb(67, 67, 67)"><strong><span class="sqsrte-large">Primary Results</span></strong></span><span style="color: rgb(102, 102, 102)">Brief Summary: </span><span style="color: rgb(34, 34, 34)">Four study designs, two blocked and two event related, were studied across multiple degrees of spatial smoothing, different cluster-forming thresholds and different software packages.  Specifically </span><a href="http://www.fil.ion.ucl.ac.uk/spm/"><span style="color: rgb(17, 85, 204)">SPM</span></a><span style="color: rgb(34, 34, 34)">, FLAME 1 from </span><a href="http://fsl.fmrib.ox.ac.uk/fsl/fslwiki/"><span style="color: rgb(17, 85, 204)">FSL</span></a><span style="color: rgb(34, 34, 34)">, OLS from FSL, 3dttest from </span><a href="https://afni.nimh.nih.gov/afni"><span style="color: rgb(17, 85, 204)">AFNI</span></a><span style="color: rgb(34, 34, 34)">, 3dMEMA from AFNI, and a permutation-based approach implemented using the </span><a href="https://www.nitrc.org/projects/broccoli/"><span style="color: rgb(17, 85, 204)">BROCCOLI software</span></a><span style="color: rgb(34, 34, 34)"> </span><a href="https://www.nitrc.org/projects/broccoli/"><span style="color: rgb(34, 34, 34)">w</span></a><span style="color: rgb(34, 34, 34)">e</span><a href="https://www.nitrc.org/projects/broccoli/"><span style="color: rgb(34, 34, 34)">r</span></a><span style="color: rgb(34, 34, 34)">e</span><a href="https://www.nitrc.org/projects/broccoli/"><span style="color: rgb(34, 34, 34)"> </span></a><span style="color: rgb(34, 34, 34)">s</span><a href="https://www.nitrc.org/projects/broccoli/"><span style="color: rgb(34, 34, 34)">t</span></a><span style="color: rgb(34, 34, 34)">u</span><a href="https://www.nitrc.org/projects/broccoli/"><span style="color: rgb(34, 34, 34)">d</span></a><span style="color: rgb(34, 34, 34)">i</span><a href="https://www.nitrc.org/projects/broccoli/"><span style="color: rgb(34, 34, 34)">e</span></a><span style="color: rgb(34, 34, 34)">d.</span> The main result, highlighted in the <a href="http://www.pnas.org/content/early/2016/06/27/1602413113/F1.expansion.html"><span style="color: rgb(17, 85, 204)">first figure</span></a> of the paper, shows that when using a parametric approach, a cluster defining threshold of p=0.01 leads to poor control of FWE (FWE from approximately 4-50%).  However, FWE control is improved when a cluster defining threshold of p=0.001 is used instead, regardless of software package used (FWE ranges from approximately 0-25%).  The more conservative nonparametric approach controls FWE regardless of cluster defining threshold in most cases, although elevated FWE were observed for the one-sample t-test in some cases due to skewed data. The <a href="http://www.pnas.org/content/early/2016/06/27/1602413113/F2.expansion.html"><span style="color: rgb(17, 85, 204)">second result</span></a>, which is the source of the 70% FWE that has appeared in many other blog posts, occurs when simply using a cluster size of 10 as an ad-hoc inference procedure.  In this case, a cluster defining threshold of p=0.001 was used and clusters with 10 or greater voxels are identified as significant. The high FWE of this approach indicates that it should not be thought of as controlling FWE. More details and the explanation of why FLAME1 appears conservative in both of these results are in the next section. The general conclusion is that when using cluster-based thresholding, a cluster-defining threshold of p=0.001 has better control of FWE than p=0.01 for SPM, FSL and AFNI. The nonparametric-based approach has better controlled FWE in the scenarios tested here.<span style="color: rgb(102, 102, 102)"><span class="sqsrte-large">Detailed Summary:</span></span><span style="color: rgb(34, 34, 34); font-weight: 700">AFNI problem identified.  </span><span style="color: rgb(34, 34, 34)">The results presented in this manuscript include the use of a pre May 2015 version of AFNI, specifically the 3dClustSim function used to implement the parametric FWE control. One of the discoveries made during this project was the smoothness estimate used in this older version of 3dClustSim had a flaw that increased the FWE. This was fixed by the AFNI developers in versions after May 2015.  Although the new version reduces FWE, it is still inflated above the target of 5%; the p=0.01 and p=0.001 cluster defining thresholds’ FWE with 3dClustSim changed from 31.0% to 27.1% and 11.5% to 8.6%, respectively.  </span><span style="color: rgb(34, 34, 34); font-weight: 700">Is FLAME1 superior? </span><span style="color: rgb(34, 34, 34)">Some results appear to support the claim that FLAME1 option in FSL has better FWE control, even in the ad-hoc case, but this is due to a known problem where FLAME1 sometimes overestimates the variance. To clarify, FLAME1 differentially weights the contribution of each subject according to the subject-specific mixed effects variance, which is a sum of within- and between-subject variances. The result is that more variable subjects contribute less to the statistic estimate. In comparison, the OLS option in FSL treats all subjects equally (also true for SPM, AFNI’s 3dttest and permutation tests). When the true between-subject variance is small, FLAME1 overestimates it, causing an increase in p-values, which reduces the FWE. When the true between subject variance is not close to 0, FLAME1 results in a more accurate estimate of the variance but the FWE can then be inflated with results similar to FSL’s OLS. The resting state data have a low true between-subject variance, leading to lower FWE than we might see with task data where systematic differences in task performance might indeed yield the predicted large between-subject differences. This is supported by a secondary simulation using task fMRI data with randomly assigned groups that found FLAME1 to have error rates comparable to FSL’s OLS. Overall, this implies that the FWE will be controlled if the true between-subject variance is small and will be elevated similarly to OLS if the variance is larger than 0.</span><span style="color: rgb(34, 34, 34); font-weight: 700">Why do parametric methods fail? </span><span style="color: rgb(34, 34, 34)">The assumptions of random field theory include that the spatial smoothness must be constant across the brain and the spatial autocorrelation follows a squared exponential distribution. The spatial autocorrelation distribution was not found to follow the squared exponential very well, instead the accuracy of the distribution varied according to distance. Simply put, if voxels were close together, there was a stronger agreement between the empirical and theoretical spatial correlation, but the two do not match for voxels that are far apart from each other. This explains why results improve for more stringent cluster forming thresholds, since clusters are smaller, hence the voxels involved are closer and the assumptions are more closely met.</span><span style="color: rgb(67, 67, 67)"><strong><span class="sqsrte-large">The Takeaways</span></strong></span><span style="color: rgb(34, 34, 34)">Should we all panic and give up on fMRI?  Are all 40,000 fMRI studies of the past worthless?  Of course not.  The </span><a href="http://blogs.warwick.ac.uk/nichols/entry/bibliometrics_of_cluster/"><span style="color: rgb(17, 85, 204)">blog post by Tom Nichols</span></a><span style="color: rgb(34, 34, 34)"> refines this estimate to a more reasonable number of studies from the past that may be impacted: closer to 3,500. (Note: PNAS has accepted an Erratum from the authors that revises the sentences that led to the sensationalized press articles.) The study shows that (a) FWE control does not work properly in the parametric tests using an ad hoc threshold of 10 voxels; (b) FWE is often controlled by permutation-based testing; (c) cluster inference for SPM, FSL, and AFNI using a cluster-defining threshold of 0.01 is likely problematic; (d) although improvements would be expected if a cluster forming threshold of 0.001 was used, FWE is still not controlled at the nominal level of 5% under all conditions.  </span><span style="color: rgb(34, 34, 34); font-weight: 700"><span class="sqsrte-large">How shall we proceed to analyze fMRI data? </span></span><span style="color: rgb(34, 34, 34)"><span class="sqsrte-large">  </span>Both parametric and nonparametric-based inference have pros and cons and work well when their assumptions are met. Prior work has highlighted the assumptions of the parametric cluster-based thresholding approach, including using a small p-value based cluster defining threshold (see</span><a href="http://www.ncbi.nlm.nih.gov/pubmed/?term=Assessing+the+significance+of+focal+activations+using+their+spatial+extent."><span style="color: rgb(34, 34, 34)"> </span><span style="color: rgb(17, 85, 204); font-weight: 700">Friston et al. (1994)</span></a><span style="color: rgb(34, 34, 34)"> and</span><a href="http://www.ncbi.nlm.nih.gov/pubmed/14683734"><span style="color: rgb(34, 34, 34)"> </span><span style="color: rgb(17, 85, 204); font-weight: 700">Hayasaka and Nichols (2003)</span></a><span style="color: rgb(34, 34, 34)"> for examples). Although it was clear the threshold needed to be low, without knowing the true spatial covariance structure, it wasn’t necessarily clear how low for real fMRI data. Since the Eklund et al. work used real fMRI data in the simulations we now know that p=0.01 is not low enough and p=0.001 is a better option.  Generally, the permutation test has fewer assumptions and tends to have better FWE control, but Eklund et al. did find some cases with the 1-sample t-test where the nonparametric approach had elevated FWE, due to skew in the data. Permutation-based options can be implemented on any NIfTI file using </span><a href="http://www2.warwick.ac.uk/fac/sci/statistics/staff/academic-research/nichols/software/snpm"><span style="color: rgb(17, 85, 204)">SnPM</span></a><span style="color: rgb(34, 34, 34)"> in SPM, </span><a href="http://fsl.fmrib.ox.ac.uk/fsl/fslwiki/Randomise"><span style="color: rgb(17, 85, 204)">randomise</span></a><span style="color: rgb(34, 34, 34)"> in FSL, </span><a href="http://fsl.fmrib.ox.ac.uk/fsl/fslwiki/PALM"><span style="color: rgb(17, 85, 204)">PALM</span></a><span style="color: rgb(34, 34, 34)"> (also affiliated with FSL), Eklund’s </span><a href="https://github.com/wanderine/BROCCOLI"><span style="color: rgb(17, 85, 204)">BROCCOLI</span></a><span style="color: rgb(34, 34, 34)"> package and </span><a href="https://surfer.nmr.mgh.harvard.edu/fswiki/mri_glmfit"><span style="color: rgb(17, 85, 204)">mri_glmfit-sim</span></a><span style="color: rgb(34, 34, 34)"> in FreeSurfer.  Importantly, AFNI users should update their versions to ensure use of either the repaired 3dClustSim (after May 2015), or the new </span><a href="https://afni.nimh.nih.gov/pub/dist/doc/program_help/3dFWHMx.html"><span style="color: rgb(17, 85, 204)">3dFWHMx</span></a><span style="color: rgb(34, 34, 34)"> function which uses a more accurate spatial smoothness estimate and will improve FWER control.  Also, using the ad hoc cluster size of 10 voxels has the largest FWE and is not recommended as a method for controlling FWE.The work of Eklund et al. supplies important information to those who choose to control the multiple comparison problem according to the FWE. In future work, researchers intending to use FWE correction can make better choices to ensure the true level of FWE is closer to the goal FWE. Although some previously published studies may have not used as stringent FWE control when they had intended to, the results can still be interpreted, but with more caution. Multiple comparison correction is just one element of neuroimaging practice, and there are countless choices in the design, acquisition, analysis and interpretation of any study. We encourage everyone to consult the OHBM Committee on Best Practice in Data Analysis and Sharing (COBIDAS) report on MRI, and review the detailed checklists for every stage of a study. The report is available directly on the OHBM website </span><a href="http://www.humanbrainmapping.org/COBIDASreport"><span style="color: rgb(17, 85, 204)">http://www.humanbrainmapping.org/COBIDASreport</span></a><span style="color: rgb(34, 34, 34)"> and </span><a href="http://biorxiv.org/content/early/2016/05/20/054262"><span style="color: rgb(17, 85, 204)">on bioRxiv</span></a><span style="color: rgb(34, 34, 34)">.  </span></p>
