<p><span style="font-weight: 700">Written by</span>: <a href="../contributors.html" target="_blank">Claude Bajada</a>, <a href="https://www.cbs.mpg.de/person/attar/374227" target="_blank">Fakhereh Movahedian Attar</a>, <a href="../contributors.html" target="_blank">Ilona Lipp</a><br><span style="font-weight: 700">Expert reviewers</span>: <a href="https://www.fz-juelich.de/SharedDocs/Personen/INM/INM-7/EN/Wagner_a.html?nn=654270" target="_blank">Adina Wagner</a>, <a href="https://www.ed.ac.uk/profile/dr-cyril-pernet" target="_blank">Cyril Pernet</a><br><span style="font-weight: 700">Newbie editors</span>: <a href="https://www.linkedin.com/in/yana-dimech-623347124/?originalSubdomain=mt" target="_blank">Yana Dimech</a>, <a href="https://www.cbs.mpg.de/person/101048/2482" target="_blank">Renzo Torrecuso</a></p>
<p>This post is about good neuroimaging practices. ‘Practices’ relates to all aspects of conducting research. By ‘good’, we mean beneficial to the field and neuroimaging community - but you’ll see that most of these practices also benefit the individual researcher. Here, we collected a number of tools, tips and tricks to do neuroimaging in the ‘best’ way possible. We aim to provide an overview and answer some questions you may have asked yourself about reproducibility and good neuroimaging practices. As usual, we refer to OHBM On-Demand videos from the educational sessions of previous annual meetings. OHBM has both a <a href="https://www.humanbrainmapping.org/m/pages.cfm?pageid=3712"><span style="color: #15c">special interest group (SIG) for Open Science</span></a> as well as a <a href="https://www.humanbrainmapping.org/i4a/pages/index.cfm?pageid=3313" target="_blank">Best Practices Committee</a>, where leading brain mappers promote and help implement Open Science and good practices in data analysis and sharing. Both the Open Science SIG and the Best Practices Committee regularly create invaluable resources, such as <a href="https://github.com/ohbm/hackathon2020" target="_blank">the annual Hackathon workshops</a>, and the <a href="https://www.humanbrainmapping.org/i4a/pages/index.cfm?pageid=3728" target="_blank">COBIDAS Best Practices in MRI and M/EEG data analysis papers</a>.</p>
<p><span style="font-weight: 700">Isn’t the main issue in our field reproducibility? Or the lack of it? Should I care about my science being reproducible?</span></p>
<p>Those are loaded questions. We think we just might not answer them because you are luring us into a trap that begins with seemingly innocent questions and then rabbit into an unending borough. There are so many terms to wade through that the novice neuroscientist can easily get lost in this bog!</p>
<p>In his video, Cyril Pernet clarifies the often used terms 'repeatability’ and ‘reproducibility’ (from <a href="https://youtu.be/WPKAcAxw96c?t=67"><span style="color: rgb(17, 85, 204)">min. 1:07</span></a>). First, ‘repeatability’ means “simply” that redoing the same analysis with the same data should result in an identical result as the original analysis, which is not as trivial as it seems. The software version and the operating system can be variables that affect the output of your imaging analysis. That, however, is only step one. In his video, David Kennedy (from <a href="https://youtu.be/wrkPoCAwkWw?t=234"><span style="color: rgb(17, 85, 204)">min. 3:54</span></a>) highlights that ‘reproducibility’ is really a spectrum. We could use the exact same data and nominally similar analysis. Or, we may have nominally similar data with the exact same analysis. Or, we may have nominally similar data with nominally similar analysis. This way we can test the sensitivity and stability of our experiment.</p>
<p style="display: block"><em><a href="https://youtu.be/WPKAcAxw96c?t=317"><span style="color: #15c">​Cyril</span></a> explaining the different levels of reproducibility. </em></p>
<p>But this leads back to your question. Scientific findings should generalise. They should first be valid (repeatable) but should also be robust to various permutations of the data and analyses used. There is a great video by Kirstie Whitaker on <a href="https://www.youtube.com/watch?v=NDNYPDm1-2c&amp;app=desktop"><span style="color: #15c">YouTube</span></a> that tackles these issues. </p>
<p><span style="font-weight: 700">The reproducibility crisis is often associated with the field of psychology, is there anything different in the field of human brain mapping?</span></p>
<p>Ok, so here we are generally talking about the more general “reproducibility”, not just about being robust to permutations. We will assume that researchers have already ensured that their analysis is re-executable. </p>
<p>In 2005, John Ioannidis published a landmark <a href="https://journals.plos.org/plosmedicine/article%3Fid%3D10.1371/journal.pmed.0020124"><span style="color: #15c">article</span></a> with the eye watering title of “Why Most Published Research Findings Are False.” If you are interested in understanding why many scientific articles are not reproducible we strongly recommend reading this article; it is an easy and insightful read. Notice that this article does not even specifically refer to psychology or to neuroimaging. This problem is general to, at least, the wider “medically-related” field.</p>
<p>The article points out that effect sizes in these fields tend to be low and that sample sizes are frequently lower than what would be needed to test for such small effects. In neuroimaging, there are many steps and expertise (and often money) involved in acquiring good data. As a result, our sample sizes tend to be typically small. Indeed, it was not too long ago when most neuroimaging articles were published on samples of approximately 20 participants. In 2020, studies with several hundred, up to a couple of thousand, participants are becoming more common, but these require a massive investment in resources and tight collaboration between sites.</p>
<p>In his video, Cyril provides an overview of cognitive biases that can contribute to limited reproducibility of neuroscientific research (from <a href="https://youtu.be/WPKAcAxw96c?t=438"><span style="color: #15c">min. 7:18</span></a>). He also explains how the analytical flexibility in neuroimaging research (such as fMRI analyses) adds an additional level of complexity (from <a href="https://youtu.be/WPKAcAxw96c?t=959"><span style="color: #15c">min. 15:59</span></a>). While papers with hot stories and “positive results” have it much easier to find a home in very high impact journals, the drawbacks of this trend are slowly starting to be recognized. Neuroimaging scientific societies are becoming aware of the importance of reproducible research and are incentivising the work. OHBM has a yearly replication award that was won by <a href="http://www.altmann.eu/"><span style="color: #15c">Andre Altmann</span></a> this year. Also, initiatives, such as DORA, <a href="https://sfdora.org/"><span style="color: #15c">The Declaration on Research Assessment</span></a>, aim to find ways of evaluating research and researchers that go beyond journal impact factors.</p>
<p><em><a href="https://youtu.be/4Y3hzSfXkbo?t=468"><span style="color: #15c">Pia Rotshtein </span></a>discussing the conflict of interest between good science and researcher’s careers.</em></p>
<p><span style="font-weight: 700">So what can we do to make neuroimaging research more reproducible?</span></p>
<p>Well, some things are harder to deal with than others. Running neuroimaging studies is time-consuming and expensive, there is very little that can be done about that, at least in the short to medium term. One thing we can do is to work towards using robust and valid measures from neuroimaging data. In his video, Xi-Nian explains how validity of our measures depends on reliability (from <a href="https://youtu.be/rpWOVmwFcz8?t=340"><span style="color: #15c">min. 5:40</span></a>). He introduces reliability indices (the intraclass correlation coefficient) and gives an example of how they can inform the extent to which inter-subject variability (which is often what we are interested in, e.g. when investigating different groups of people or brain-behaviour correlations) exceeds intra-subject variability (which in these cases is unwanted variability in repeated measurements, often caused by measurement noise). He reminds us of this <a href="https://pubmed.ncbi.nlm.nih.gov/26158964/"><span style="color: #15c">paper</span></a> pointing out that brain-behaviour correlations are “puzzling high”, given the <a href="https://link.springer.com/article/10.3758/s13428-017-0935-1"><span style="color: #15c">reliability of our cognitive measures</span></a> and of our imaging measures. From <a href="https://youtu.be/rpWOVmwFcz8?t=980"><span style="color: #15c">min. 16:20</span></a> he goes through a variety of imaging measures and their reliability, and introduces <a href="http://fcon_1000.projects.nitrc.org/indi/CoRR/html/"><span style="color: #15c">CoRR</span></a> (<a href="https://youtu.be/rpWOVmwFcz8?t=1290"><span style="color: #15c">min. 21:30</span></a>), the Consortium for Reliability and Reproducibility. The prerequisite to have reliable imaging measures is, of course, to have sufficient data quality.</p>
<p><span style="font-weight: 700">How do I ensure that my data exhibits sufficient quality?</span></p>
<p>Quality assurance (QA) and quality control (QC) procedures are put forward to ensure and verify the quality of neuroimaging data, respectively. Although somewhat intertwined, QA and QC are slightly different. QA is process-oriented and aims to boost our confidence in the data via routine system checks, whereas QC is product-oriented and deals with verifying the quality of the final product in the pipeline. In his video, Pradeep Raamana briefly introduces QA and QC and outlines the different QC steps involved in the acquisition of neuroimaging data (from min. <a href="https://youtu.be/HrGccuOPUIA?t=227"><span style="color: #15c">3:47</span></a>). Visualising and checking your neuroimaging data at all processing stages is absolutely essential. The most important yet basic tool you need is therefore an image viewer that allows simultaneous visualization of the three image planes, and of course, you as the observer! For more specialized QC, Pradeep presents a list of some of the available neuroimaging QC tools per neuroimaging modality <a href="https://www.pathlms.com/ohbm/courses/12238/sections/15846/video_presentations/138114"><span style="color: #15c">here</span></a>, where he also presents use-cases of some of the tools. </p>
<p>In order to conduct QC successfully, one would need to take care of the various common types and sources of artifacts encountered in neuroimaging data. Importantly, we need to keep in mind that QA and QC must be tailored to the specific nature of neuroimaging data in its various modalities, separately. </p>
<p>In the videos of the <a href="https://www.pathlms.com/ohbm/courses/12238/sections/15846/video_presentations/137528"><span style="color: #15c">‘Taking Control of Your Neuroimaging Data’</span></a> session, some of these procedures are presented. Pradeep introduces common sources of artifacts in anatomical MRI (<a href="https://youtu.be/HrGccuOPUIA?t=494"><span style="color: #15c">min. 8:14</span></a>) and presents some tips and tricks for detecting artifacts in T1-weighted images (<a href="https://youtu.be/HrGccuOPUIA?t=1148"><span style="color: #15c">min. 19:08</span></a>). Then, Martina Callaghan presents key metrics to perform scanner QA for functional MRI, emphasising the need to look for subtleties (<a href="https://youtu.be/srCkKgy5kCQ?t=233"><span style="color: #15c">min. 3:53</span></a>). Here, the key is to establish whether the system fluctuations inherent in the acquisition procedure and hardware are sufficiently low to allow detection of BOLD-related signal changes in task-based and resting-state functional MRI. Martina Callaghan then presents some of the online (i.e. real-time) QC procedures for functional MRI (<a href="https://youtu.be/srCkKgy5kCQ?t=1037"><span style="color: #15c">min. 17:17</span></a>). </p>
<p>Esther Kuehn then takes over and introduces artifacts in high resolution functional MRI acquired at high-field strength with particular emphasis on cortical layer imaging applications and presents some available means of artifact-correction (<a href="https://youtu.be/bixSyDq6dvM?t=1"><span style="color: #15c">from beginning</span></a>). In her <a href="https://youtu.be/pfpWYFNtjko"><span style="color: #15c">video</span></a>, Joset Etzel introduces a different aspect of QC for neuroimaging data - dataset QC - and talks about the importance of checklists and standard operating procedures (SOPs). </p>
<p>Dataset QC aims to verify whether a valid dataset (i.e. one that has already passed the various data QC steps) is also usable by different people at different times in different places, and intuitive data organisation alone is not sufficient. Finally, in his <a href="https://youtu.be/6kH6t3J-EdU?t=2"><span style="color: #15c">video</span></a>, Alexander Leemans introduces common artifacts in diffusion MRI, presents strategies for checking the quality of data and common errors in this checking, and also correcting artifacts. </p>
<p><span style="font-weight: 700">I’ve got so much data, how do I organise it?</span></p>
<p>Lots of neuroimaging data are acquired all over the world and the resulting datasets are organized in different ways according to the personal preferences of the users or the labs. With Open Data, so data that is publicly accessible, picking up momentum, there is growing need for standardization of neuroimaging datasets so that they are easy to use soon across a wide community of neuroscientists. The brain imaging data structure (BIDS) initiative aims to standardize neuroimaging data structures in order to make them interoperable under the FAIR data principle. In <a href="https://youtu.be/fhqIPyU9Mm4?t=1448"><span style="color: #15c">this</span></a> tutorial, the BIDS data structure is introduced as a practical means for achieving FAIR data. Here, a number of BIDS resources and repositories and simple BIDS specifications are also given for an easy get-go <a href="https://youtu.be/fhqIPyU9Mm4?t=1647"><span style="color: #15c">(min. 27:27)</span></a>. Later, a hands-on session on how to create and validate a basic BIDS dataset is also introduced <a href="https://youtu.be/fhqIPyU9Mm4?t=2097"><span style="color: #15c">(min. 34:57)</span></a>. Also check out the <a href="https://www.youtube.com/watch?v=K9hVAr5fvJg"><span style="color: #15c">TrainTrack session on BIDS</span></a> of this year’s virtual meeting by Sam Nastase!</p>
<p><em><a href="https://youtu.be/fhqIPyU9Mm4?t=1556"><span style="color: #15c">Jeffrey</span></a> going through the benefits of the brain imaging data structure (BIDS).</em></p>
<p>Once you have nicely organised your data, they are also easier to use for other people. To make neuroimaging more reproducible overall, something else that can be done is to ensure that data does not get lost and forgotten. In short that our data are Findable, Accessible Interoperable and Reusable (or FAIR; see the educational course on FAIR data from <a href="https://youtu.be/fhqIPyU9Mm4?t=112"><span style="color: #15c">min. 1:52 </span></a>by Maryann Martone and Jeffrey Grethe).</p>
<p><em><a href="https://youtu.be/fhqIPyU9Mm4?t=676"><span style="color: #15c">The FAIR principles</span></a>.</em></p>
<p>This way, your science will be more robust, transparent and verifiable.</p>
<p>The problem is that making research FAIR as an afterthought is really tough. Indeed, generating or curating good quality data that abides by FAIR principles requires some forethought (FAIR workshop <a href="https://youtu.be/fhqIPyU9Mm4?t=756"><span style="color: #15c">min. 12:36</span></a>). Not only do a lot of steps and expertise go into acquiring good quality data, but your data need to be in a format and in a place that makes those data easy to use for your present self, your future self and for someone who is not yourself! </p>
<p>One tool to share statistical maps from your study is the platforms <a href="https://neurovault.org/"><span style="color: #15c">NeuroVault</span></a> and <a href="https://neurosynth.org/"><span style="color: #15c">Neurosynth</span></a>. In his video, Chris Gorgolewski goes through the advantages that uploading your map has for you, such as the options for fancy visualisations of your maps (<a href="https://youtu.be/TKgYTKg--Qk?t=277"><span style="color: #15c">min. 4:37</span></a>), cognitive decoding of your maps (<a href="https://youtu.be/TKgYTKg--Qk?t=325"><span style="color: #15c">min. 5:25</span></a>), search to find similar maps in papers (<a href="https://youtu.be/TKgYTKg--Qk?t=385"><span style="color: #15c">min. 6:25</span></a>), gene decoding (<a href="https://youtu.be/TKgYTKg--Qk?t=424"><span style="color: #15c">min. 7:04</span></a>).</p>
<p><span style="font-weight: 700">How can I make sure that my analysis workflow can be reproduced by others?</span></p>
<p>If you want all aspects of your study to be documented and reproducible, then this of course also includes your analysis. The BIDS structure can help with setting up a reproducible workflow, but it is not sufficient. It also needs to be clear which processing steps have happened, which analyses were done, with which software and which parameters, etc. There are a lot of tools out there to help you and the Center for Reproducible Neuroimaging Computation initiative (ReproNim) has held an <a href="https://github.com/ReproNim/ohbm2018-training"><span style="color: #15c">extensive course</span></a> at the 2018 annual meeting about this (and a <a href="https://www.repronim.org/how-would-repronim.html"><span style="color: #15c">whole Webinar series</span></a> on best practices for neuroimaging, if you are interested). </p>
<p>Starting with the “computational basis”, Yaroslav Halchenko gives an introduction into the Linux shell, including the importance of environment variables (from <a href="https://youtu.be/2Qm4krjaSfM?t=770"><span style="color: #15c">min. 12:50</span></a>) to ensure you are running the right version of software, how to use shell history (from <a href="https://youtu.be/2Qm4krjaSfM?t=1420"><span style="color: #15c">min. 23:40</span></a>) to check whether you indeed ran the right commands, and how to write shell scripts (<a href="https://youtu.be/2Qm4krjaSfM?t=1770"><span style="color: #15c">min. 29:30</span></a>). He also shows how Neurodebian can be used to search and download software (<a href="https://youtu.be/2Qm4krjaSfM?t=2481"><span style="color: #15c">min. 41:21</span></a>). </p>
<p>Most people have probably heard the name Git before. (Did you know the official definition is <a href="https://git-scm.com/docs/git.html"><span style="color: #15c">“stupid content tracker”</span></a>?) Yaroslav explains the Git philosophy in 2 minutes (<a href="https://youtu.be/2Qm4krjaSfM?t=3481"><span style="color: #15c">min. 58:01</span></a>) and shows the most important commands (<a href="https://youtu.be/2Qm4krjaSfM?t=3170"><span style="color: #15c">min. 52:50</span></a>). While Git is useful to keep track of your scripts, get and provide code, a tool called DataLad (<a href="https://youtu.be/2Qm4krjaSfM?t=3797"><span style="color: #15c">min. 1:03:17</span></a>) can be used to do similar stuff with datasets. A hands-on session on this is provided in the Workflows for neuroimaging session from <a href="https://youtu.be/kl9aiG0LBTA?t=2840"><span style="color: #15c">min. 47:20</span></a>, and how this can be combined with specific statistical analyses is explained from <a href="https://youtu.be/kl9aiG0LBTA?t=6728"><span style="color: #15c">min. 1:52:08</span></a>. </p>
<p>Other tools to help you make sure you use consistent software within a study are containers and virtual machines. Dorota Jarecka gives a good overview of why these are very useful in research (from <a href="https://youtu.be/kl9aiG0LBTA?t=459"><span style="color: #15c">min. 7:39</span></a>) and even guides you through some exercises (from <a href="https://youtu.be/kl9aiG0LBTA?t=945"><span style="color: #15c">min. 15:45</span></a>).  Jean-Baptiste Poline gives a short intro to Jupyter notebooks to demonstrate your code to others (from <a href="https://youtu.be/kl9aiG0LBTA?t=9831"><span style="color: #15c">min. 2:43:51</span></a>).</p>
<p>This year’s OHBM <a href="https://ohbm.github.io/hackathon2020/"><span style="color: #15c">Hackathon</span></a> also has a session on <a href="https://www.youtube.com/watch?v=ir_ILsjkwss"><span style="color: #15c">Git</span></a> by Steffen &amp; Saskia Bollman, on <a href="https://www.youtube.com/watch?v=AWfrlKTLkqw"><span style="color: #15c">good coding practices with Matlab</span></a> by Agah Karakuzu, on <a href="https://www.youtube.com/watch?v=QsAqnP7TwyY"><span style="color: #15c">Datalad</span></a> by Adina Wagner and on <a href="https://www.youtube.com/watch?v=pc3YOZUG3lQ"><span style="color: #15c">Containers</span></a> by Tom Shaw and Steffen Bollmann.</p>
<p><span style="font-weight: 700">You said that replicability also refers to other people being able to get the same outcome as my study, but if they test different participants, this is out of my control, right?</span></p>
<p>This is a good point, it is somewhat out of your control, but there are some ways in which you can help. First, being very transparent about what you did to your data will allow others to adapt methods as similar as possible to yours. As Celia Greenwood explains (from <a href="https://youtu.be/kl9aiG0LBTA?t=8641"><span style="color: #15c">min. 2:24:01</span></a>), the final statistical measure that one tries to replicate involves a lot more than just the statistical test, but includes all steps before, the processing, exclusion of outliers etc., which sometimes makes it hard to even work out what the null hypothesis is. She states that reproducibility in the statistical sense is about the final inference you make, so it is tied to the p-value. And this of course depends on your sample size and, to some extent, chance. In a demonstration (from <a href="https://youtu.be/kl9aiG0LBTA?t=9264"><span style="color: #15c">min. 2:34:24</span></a>) she shows that if you draw different samples from the same population, there is huge variability in the p-values and effect sizes that you get across samples (even with sample sizes of N &gt; 100) , which are purely a result of random sampling.</p>
<p><em><a href="https://youtu.be/kl9aiG0LBTA?t=9536"><span style="color: #15c">Celia</span></a> illustrates the effect of random sampling on estimated effect sizes.</em></p>
<p><span style="font-weight: 700">Is this why “most published research findings are false?”</span></p>
<p>Are you insisting on going back to things we have already discussed?! I suppose it is fair to say that there is more to it. A measure called “predictive value” is the probability of the alternative hypothesis being true given your test result. In his video, Jean-Baptiste (from <a href="https://youtu.be/kl9aiG0LBTA?t=10034"><span style="color: #15c">min. 2:47:14</span></a>) uses a Jupyter notebook to explain the Bayesian math behind this value and shows that this measure depends on the power of your study as well as the odds ratio of the alternative hypothesis over the null hypothesis being true. So the lower the power in your study, the more unlikely that the alternative hypothesis (usually what you are interested in) is true, even if you have a significant result. And most neuroscience studies do not have much power, as shown by <a href="https://www.nature.com/articles/nrn3475"><span style="color: #15c">Katherine Button</span></a>. </p>
<p><span style="font-weight: 700">Well, you may say now, how do I know what my power will be? And is there even a point in doing my experiment or will it just produce another - false - research finding!?. </span></p>
<p>Good question. Doing power analysis for neuroimaging studies is not straightforward, but luckily, some packages, such as <a href="http://fmripower.org/"><span style="color: #15c">fmripower</span></a> and <a href="http://neuropowertools.org/"><span style="color: #15c">neuropower</span></a>, have been developed to at least get an educated guess of what your power might be. As Jeanette Mumford explains in her video (from <a href="https://youtu.be/wOydW8FFLl0?t=293"><span style="color: #15c">min. 4:53</span></a>) doing a power analysis has many benefits. She also gives some tips on how to assess other people’s power analyses (from <a href="https://youtu.be/wOydW8FFLl0?t=428"><span style="color: #15c">min. 7:08</span></a>) and what to consider when estimating effect sizes based on the literature (from <a href="https://youtu.be/wOydW8FFLl0?t=558"><span style="color: #15c">min. 9:18</span></a>). Jeanette also explains why the difficulty of doing power analysis increases with difficulty in model (from <a href="https://youtu.be/wOydW8FFLl0?t=719"><span style="color: #15c">min. 11:59</span></a>).</p>
<p><em><a href="https://youtu.be/wOydW8FFLl0?t=919"><span style="color: #15c">Jeanette</span></a> talking about the power of different statistical models.</em></p>
<p><span style="font-weight: 700">What else can I do to ensure best practices in neuroimaging?</span></p>
<p>Thorough reporting of what you have been doing in your data acquisition and analysis is always a good idea. <a href="https://www.nature.com/articles/nn.4500"><span style="color: #15c">Guidelines</span></a> have been created by the <a href="https://www.humanbrainmapping.org/i4a/pages/index.cfm?pageID=3728"><span style="color: #15c">Committee on Best Practices in Data Analysis and Sharing</span></a> (COBIDAS; also see <a href="https://www.pathlms.com/ohbm/courses/5158/sections/7756/video_presentations/75751"><span style="color: #15c">Tonya White’s video</span></a> for the idea behind COBIDAS) for MRI and MEEG.</p>
<p>Various tools are available for testing <a href="https://the-turing-way.netlify.app/reproducible-research/testing.html"><span style="color: #15c">your code</span></a>. Also, if you publish your code on sites such as github, then other researchers can try it out and help further develop it. </p>
<p><a href="https://www.pnas.org/content/115/11/2600.short"><span style="color: #15c">Preregistration</span></a> and <a href="registered-reports-in-human-brain-imaging.html"><span style="color: #15c">registered reports</span></a> are becoming more and more popular for neuroimaging, meaning that more and more journals accept and encourage them. In her video, Pia Rotshtein explains the philosophy behind and principles of registered reports (from <a href="https://youtu.be/4Y3hzSfXkbo?t=666"><span style="color: #15c">min. 11:06</span></a>) and shows some examples (from <a href="https://youtu.be/4Y3hzSfXkbo?t=1375"><span style="color: #15c">min. 22:55</span></a>).</p>
<p><em>Tonya telling us about the <a href="https://www.humanbrainmapping.org/i4a/pages/index.cfm?pageID=3728"><span style="color: #15c">Committee on Best Practices in Data Analysis and Sharing</span></a>.</em></p>
<p><span style="font-weight: 700">If I get into all these things, will I still have time to do research?</span></p>
<p>That is why there are 36 hours in every day! Seriously though, this is all part of doing research! Often, however, efforts on good practices in neuroimaging are not publishable by themselves and have not been well respected. There are good reasons and incentives to follow Open Science practices as individual researchers (for examples see <a href="https://www.phdnet.mpg.de/2020-07-07-open-science-in-daily-research"><span style="color: #15c">this summary</span></a>) and with the new OHBM initiative Aperture (see <a href="https://www.pathlms.com/ohbm/courses/12238/sections/15846/video_presentations/137848"><span style="color: #15c">video</span></a> and <a href="https://www.humanbrainmapping.org/i4a/pages/index.cfm?pageID=4009&amp;activateFull=true"><span style="color: #15c">website</span></a>), a new room for unconventional research objects (such as software and documentation) is being created.</p>
<p>If this still all seems overwhelming and time consuming, don’t worry. Most of the tools presented here have been developed to save you time and resources in the long run while making your research more sustainable. Think about the time that one would spend re-acquiring a data set because of a previously unnoticed problem with the scanner, trying to make sense of not intuitively organised data or trying to find a mistake in a long, badly structured code. Putting in place some of these preventative measures, does not seem like such a big investment anymore. </p>
<p>If you’re hooked, stay tuned. The numerous emerging Open Science initiatives keep coming up with new ideas and tools for how to make research as a whole more reproducible and trustworthy, and help us brain mappers, conduct neuroimaging research in more robust and applicable ways.</p>
