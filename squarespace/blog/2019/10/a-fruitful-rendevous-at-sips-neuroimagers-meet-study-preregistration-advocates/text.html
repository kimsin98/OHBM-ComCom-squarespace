<p><span class="sqsrte-large">By Johannes Algermissen, James Bartlett, Remi Gau, Stephan Heunis, Eduard Klapwijk, Matan Mazor, Mariella Paul, Antonio Schettino, <a href="../contributors.html">David Mehler</a> </span></p>
<p>The neuroimaging field has recently seen a substantial surge in new initiatives that aim to make research practices more robust and transparent. At our annual OHBM meetings you will have likely come across the <a href="https://twitter.com/ohbm/status/1088801280787652609"><span style="color: rgb(17, 85, 204)">Open Science room</span></a>. While many aspects fall under the <a href="https://blogs.plos.org/neuro/2018/01/31/open-science-sharing-is-caring-but-is-privacy-theft-by-david-mehler-and-kevin-weiner/"><span style="color: rgb(17, 85, 204)">umbrella term Open Science</span></a>, for this post we focus on research practices that aim to make science <a href="ohbm2017-an-interview-with-russell-poldrack-making-neuroscience-more-reproducible.html"><span style="color: rgb(17, 85, 204)">more replicable and reproducible</span></a>. These include non peer-reviewed study <a href="https://cos.io/prereg/"><span style="color: rgb(17, 85, 204)">preregistration</span></a>, peer-reviewed <a href="https://blogs.scientificamerican.com/observations/introducing-registered-reports-a-new-way-to-make-science-robust/?previewid=9B58A58B-9C74-45C4-9801E355247CC625"><span style="color: rgb(17, 85, 204)">registered reports</span></a> that reward researchers’ study plan with in-principle acceptance before data collection, but also code and data sharing tools such as <a href="https://neurovault.org/"><span style="color: rgb(17, 85, 204)">NeuroVault</span></a> and <a href="https://openneuro.org/"><span style="color: rgb(17, 85, 204)">OpenNeuro</span></a>.</p>
<p>As neuroimagers, we work closely with and learn from other disciplines, including Psychology. One place where a lot of grassroot development has come to fruition in recent years is the annual meeting of the Society for the Improvement of Psychological Science (<a href="https://www.improvingpsych.org/SIPS2019/"><span style="color: rgb(17, 85, 204)">SIPS)</span></a>. SIPS breaks with the traditional conference format and focuses on practical work, peer projects and solving concrete problems in groups. The SIPS experience can feel a bit like a playground for research practice geeks: participants sit in the driver's seat and can <a href="https://docs.google.com/spreadsheets/d/1G_SoWUquak6oD-b2L3L-XB7cF97UAGb8PPyVg3r0Lts/edit#gid=0"><span style="color: rgb(17, 85, 204)">pick from a variety</span></a> of so-called unconferences where they <a href="https://docs.google.com/document/d/1bSbGtU_XL99qoFKtH7w50K6uGpsmi8yIDnb8YtstixE/edit"><span style="color: rgb(17, 85, 204)">pitch</span></a> and <a href="https://docs.google.com/document/d/1ZNQj8o2woJvLw5hbgtOjQDd9TNonsy_0gRMn8YL-reg/edit"><span style="color: rgb(17, 85, 204)">debate</span></a> ideas to reform research practices, hackathons where everyone can contribute their “<a href="https://docs.google.com/document/d/1DKhnypsG__XG9k_16smU3IJDYGgnxFP5LHw4P6Qh50g/edit"><span style="color: rgb(17, 85, 204)">bits</span></a>” and <a href="https://docs.google.com/document/d/14jdh7t79OBanDzN9q0lH0xQKBJPWM6l87ks7cnjWMO8/edit"><span style="color: rgb(17, 85, 204)">thoughts</span></a>, and workshops where you can catch up on learning to use the latest <a href="https://docs.google.com/document/d/1tKoUkNmocevC2LpXyKeO4hNbAEI1P87x-_4K-lvlHFQ/edit"><span style="color: rgb(17, 85, 204)">R packages</span></a> or <a href="https://docs.google.com/document/d/1HTS2vkUsaP1pyqsAOjnRnlXRG6cF80FBr_3yHG50bKs/edit"><span style="color: rgb(17, 85, 204)">Bayesian analysis</span></a>. In this vibrant setting we embarked as a group of enthusiastic neuroimagers on an expedition to intermingle with other open science crowds. We wanted to find out how study preregistration and registered reports could be tailored more towards neuroimaging studies. Prepared with a list of challenges that we learned about through our <a href="https://docs.google.com/forms/d/e/1FAIpQLSe9IAMFThhpPday_tHvkh1dTGCqjcdKpeb1IspuTMvU48eCPw/viewform"><span style="color: rgb(17, 85, 204)">informal survey</span></a>, we felt determined to provide more clarity around adequate statistical power in our field, and strived to ultimately come up with a potential user-friendly template for preregistration of neuroimaging studies. We completed some initial steps at the hackathon and the immediate aftermath with a focus on tools that help researchers preregister their studies. Here, we summarize our group projects and provide you with some (interim) outcomes. </p>
<p><span class="sqsrte-large"><span style="font-weight: 700">Collection of preregistrations and registered reports in neuroimaging</span> </span><br>Preregistration and registered reports are ways to state in advance what your hypothesis is and how you are planning to run and analyze the study. They are meant as tools to prevent researchers’ own cognitive bias (e.g., hindsight bias or confirmation bias) hijacking their investigation. They are not meant to stifle exploration but to make very explicit what part of a study was confirmatory and what part was exploratory (see <a href="http://cos.io/prereg/"><span style="color: rgb(17, 85, 204)">http://cos.io/prereg/</span></a> and <a href="http://cos.io/rr/"><span style="color: rgb(17, 85, 204)">http://cos.io/rr/</span></a> for more details). Preregistration protocols have been around for a while for clinical trials but they have only started in the past few years to be on the radar of psychology researchers. The uptake seems to have been much slower in research involving (f)MRI, EEG, or MEG. Apart from the large amount of methodological and analytical detail needed to preregister neuroimaging studies, one reason may be the lack of examples of what a preregistration in those fields could look like. Those M/EEG and fMRI preregistrations and registered reports scattered on the internet are also hard to find. Therefore, during the hackathon, we started a list of all the openly available neuroimaging preregistrations and registered reports. This resulted in a <a href="https://docs.google.com/spreadsheets/d/1_R3UBKvT4vEDOAPXxR8W98QGvZMhpuyxlvYzIGekf0w/edit?usp=sharing"><span style="color: rgb(17, 85, 204)">spreadsheet</span></a>, accompanied by keywords to make it easier to select relevant ones you are interested in. This document is still a work in progress and we welcome contributions to this potentially ever-growing list, especially if we missed one of your own preregistrations! Simply use this <a href="https://docs.google.com/forms/d/e/1FAIpQLSc_l8nTd-gq_WlC7LvpLP5GKChNhD_mok1EnPX48xD3MYzdWA/viewform?usp=sf_link"><span style="color: rgb(17, 85, 204)">form</span></a> to add an entry. We hope that such an easily accessible list of preregistrations will inspire many more neuroscientists to preregister their studies and will help to establish best practices.  </p>
<p><span class="sqsrte-large"><span style="font-weight: 700">BrainPower: resources for power analysis in neuroimaging</span> </span><br>Every planning phase of an empirical neuroimaging research project should consider sample size and statistical power: How big is the effect that I am interested in? How likely am I to observe it given the resources (number of participants, number of trials) at my disposal? Power analysis should provide clarity on these questions. It might appear relatively easy for simple designs with one-dimensional behavioural variables, especially with the help of programs such as <a href="http://www.psychologie.hhu.de/arbeitsgruppen/allgemeine-psychologie-und-arbeitspsychologie/gpower.html"><span style="color: rgb(17, 85, 204)">G*Power</span></a> and standard effect size measures such as Cohen's d. However, the high-dimensional nature of neuroimaging data and designs (processing three-dimensional data over time with mass univariate and multivariate approaches) requires additional steps, e.g., cluster correction, to prevent false-positive inference. And our understanding of "effects" based on these data and methods is not necessarily as intuitive: how strong should the level of activation be, or how large should the cluster be? </p>
<p>One important approach to power analysis is simulations: When taking resting-state data and adding an activation of a certain size and extent, can I reliably find the effect? This approach has been facilitated by advances in computational power and new software in recent years, allowing researchers to have full control of the ground-truth. Alternative approaches to estimate effect sizes is relying on past literature (which may provide <a href="https://www.physiology.org/doi/full/10.1152/jn.00765.2017?url_ver=Z39.88-2003&amp;rfr_id=ori%3Arid%3Acrossref.org&amp;rfr_dat=cr_pub%3Dpubmed"><span style="color: rgb(17, 85, 204)">biased estimates</span></a>) and re-using existing, or even <a href="https://brainhack101.github.io/neurolinks/"><span style="color: rgb(17, 85, 204)">open</span></a> data sets. </p>
<p>For both approaches, experts have created primers, tools, and software. Unfortunately, their use  may not always seem intuitive. Further, researchers might have a hard time recognizing which tools best suits their specific needs. We thus collated a variety of such tools, compared these different approaches, and described their use (“how to”) to empirical researchers. Overall we gather collection that provides: </p>
<ol><li>Informative journal articles on the underlying principles of power analysis</li><li>Software toolboxes for conducting power analysis in fMRI</li><li>Tools for simulating fMRI data with which to pilot power analyses</li><li>A list of useful questions to ask or heuristics to consider when planning an experiment.</li></ol>
<p>This list of resources is <a href="https://brainpower.readthedocs.io/en/latest/index.html#"><span style="color: rgb(17, 85, 204)">openly available</span></a> and still growing in content. The immediate future goal is to expand the resources with tutorials and work examples of conducting power analyses on real and simulated fMRI data. We then plan to formalise these resources into a website. We invite and welcome any and all contributions from the community!</p>
<p><span class="sqsrte-large"><span style="font-weight: 700">A new way to calibrate the smallest effect size of interest (SESOI) for neuroimaging, using an fMRI example</span></span><br>Adequate sample size planning is crucial to make good use of resources and draw <a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0184923"><span style="color: rgb(17, 85, 204)">valid inferences</span></a> from imaging data. One-size-for-all recommended sample sizes are slowly being replaced by power analysis procedures that are based on effect sizes that seem reasonable. In the more common approach, effect sizes are estimated based on available data or previous studies. However, this approach does not account for the ability to necessarily detect a meaningful effect size. An alternative approach is to power studies sufficiently to detect the <a href="https://journals.sagepub.com/doi/full/10.1177/1745691614528520"><span style="color: rgb(17, 85, 204)">smallest effect size of interest</span></a> (SESOI), thereby increasing the chance to find an effect that is meaningful for the research question (e.g., for practical, or theoretical reasons). Also, in the event of a non-significant (i.e., “null”) finding, this approach increases the chances to <a href="https://jeps.efpsa.org/articles/abstract/10.5334/e2019a/"><span style="color: rgb(17, 85, 204)">reject negligible effect sizes</span></a>, rendering “null findings” more informative. Hence, while this approach is more rigorous, it often requires larger samples, especially when studying higher order cognitive functions where group effect sizes are known to be small. On the other hand, running too many participants also comes with a cost: scanner time is an expensive resource of limited availability. Identifying a procedure that can balance this trade-off would thus be desirable and potentially help researchers to implement a sampling plan that is based on a SESOI. </p>
<p>We thus started with the following thought experiment: in an attempt to optimize sample sizes for specific experiments and statistical tests, one can capitalize on the fact that neuroimaging data is rich and affords numerous statistical tests that are statistically orthogonal. It is safe to assume that some sources of noise are shared between contrasts, within a participant (for example, a participant that moves a lot in the scanner will have more noisy parameter estimates), and that other sources are shared between participants within the same lab (for example, the quality of the scanner). Based on these two points, we envision a dynamic procedure for sample size specification that is sensitive to the noise in the specific sample of participants. Implementing such a procedure seems fairly simple: data acquisition stops exactly when a group-level contrast that is orthogonal to the ones of interest reaches a pre-specified significance level in a pre-specified region of interest.</p>
<p><span style="font-weight: 700"><span class="sqsrte-large">A preregistration template for EEG </span></span><br>Analyzing neuroimaging data involves a myriad of decisions that researchers often consider only after data collection. When preregistering a neuroimaging study, thinking of each detail of the analysis can be challenging, especially because current available preregistration templates are generic and do not ask for the relevant technical details and specifications that are relevant for EEG experiments. For example, preprocessing EEG data involves many decisions - including resampling, filtering, and artefact rejection - that can have a profound impact on the results. </p>
<p>As part of the hackathon, we started to create a preregistration template for EEG studies that highlights such decisions during preprocessing and statistical analysis. For instance, the user is reminded to describe the electrode type and brand, data import, resampling, filtering, epoching, artefact detection/rejection/correction procedures, baseline correction, and averaging. The current version of the template is a text document based on the standard OSF preregistration form where we added specific questions about preprocessing and analysis steps for event-related potentials (ERPs). This <a href="https://docs.google.com/document/d/1RmlHcWJcWMErZzjJgz9Q0rqsy-E9Vs9DYeKdvX9cawU/edit?usp=sharing"><span style="color: rgb(17, 85, 204)">EEG preregistration template</span></a> is an ongoing project. If you have worked with EEG data or preregistrations before, your input would be highly appreciated! Ultimately, we aim to include the finished template on the <a href="https://osf.io/zab38/wiki/home/"><span style="color: rgb(17, 85, 204)">OSF list</span></a> of preregistration forms and extend the preregistration template to other analyses of EEG data (e.g., time-frequency analyses).  </p>
<p>To wrap up, SIPS certainly provides a great opportunity for neuroimagers to intermingle with others and contribute to projects related to scientific practices in an open, inclusive, and dynamic environment. Anyone can pitch a session ad-hoc for the next day and the outcome of each project is <a href="https://osf.io/ndzpt/"><span style="color: rgb(17, 85, 204)">openly documented on the OSF</span></a>. This ensures that projects like ours on preregistration and neuroimaging can develop and live a happy after-conference life. </p>
